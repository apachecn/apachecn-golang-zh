# 收集和组织数据

民意调查显示，数据科学家 90%或更多的时间用于收集、组织和清理数据，而不是训练/调整他们复杂的机器学习模型。为什么会这样？机器学习不是很有趣吗？为什么我们需要如此关注数据的状态？首先，没有数据，我们的机器学习模型无法学习。这似乎是显而易见的。然而，我们需要认识到，我们构建的模型的部分优势在于我们提供给它们的数据。俗话说，*垃圾进，垃圾出*。我们需要确保收集相关、干净的数据，为我们的机器学习模型提供动力，以便它们能够按预期操作数据并产生有价值的结果。

在使用某些类型的模型时，并非所有类型的数据都适用。例如，当我们有高维数据（例如，文本数据）时，某些模型的性能不好，而其他模型假设变量是正态分布的，这肯定不总是如此。因此，我们必须注意收集适合我们用例的数据，并确保我们了解数据和模型将如何交互。

收集和组织数据占用数据科学家大量时间的另一个原因是，数据往往杂乱无章，难以聚合。在大多数组织中，数据可能以各种系统和格式存储，并具有各种访问控制策略。我们不能假设为我们的模型提供一个训练集和指定一个文件路径一样容易；事实往往并非如此。

为了形成培训/测试集或为预测模型提供变量，我们可能需要处理各种格式的数据，如 CSV、JSON、数据库表等，并且可能需要转换单个值。常见的转换包括解析日期时间、将分类数据转换为数字数据、标准化值以及跨值应用某些函数。然而，我们不能总是假设某个变量的所有值都存在或能够以类似的方式进行解析。

数据通常包括缺少的值、混合类型或损坏的值。我们如何处理这些场景将直接影响我们构建的模型的质量，因此，我们必须愿意仔细收集、组织和理解我们的数据。

尽管这本书的大部分内容将集中在各种建模技术上，但您应该始终把数据收集、解析和组织作为一个成功的数据科学项目的一个（或可能）关键组件。如果您的项目的这一部分没有以高度的完整性仔细开发，那么从长远来看，您将给自己带来麻烦。

# 处理数据-Gopher 风格

与用于数据科学/分析的许多其他语言相比，GO 为数据操作和解析提供了非常强大的基础。尽管其他语言（例如 Python 或 R）可能允许用户以交互方式快速浏览数据，但它们通常促进破坏完整性的便利性，也就是说，动态和交互数据浏览通常会导致代码在更普遍的应用中表现出奇怪的行为。

以这个简单的 CSV 文件为例：

```
1,blah1
2,blah2
3,blah3
```

的确，我们可以很快编写一些 Python 代码来解析此 CSV，并从整数列输出最大值，而不知道数据中的类型：

```
import pandas as pd

# Define column names.
cols = [
 'integercolumn',
 'stringcolumn'
 ]

# Read in the CSV with pandas.
data = pd.read_csv('myfile.csv', names=cols)

# Print out the maximum value in the integer column.
print(data['integercolumn'].max())
```

此简单程序将打印正确的结果：

```
$ python myprogram.py
3
```

现在，我们删除其中一个整数值以生成缺少的值，如下所示：

```
1,blah1
2,blah2
,blah3
```

因此，Python 程序的完整性完全崩溃；具体地说，程序仍然运行，没有告诉我们任何事情发生了变化，仍然产生一个值，并产生一个不同类型的值：

```
$ python myprogram.py
2.0
```

这是不能接受的。除了一个整数值外，所有整数值都可能消失，我们无法洞察这些变化。这可能会在我们的建模中产生深刻的变化，但很难找到它们。一般来说，当我们选择动态类型和抽象的便利性时，我们接受了这种行为的可变性。

这里重要的不是您不能在 Python 中处理此类行为，因为 Python 专家将很快认识到您可以正确处理此类行为。关键是，这种便利在默认情况下不会促进诚信，因此，很容易击中自己的脚。

另一方面，我们可以利用 Go 的静态类型和显式错误处理来确保数据按预期进行解析。在这个小示例中，我们还可以编写一些 Go 代码来解析我们的 CSV（现在不必担心细节）：

```
// Open the CSV.
f, err := os.Open("myfile.csv")
if err != nil {
    log.Fatal(err)
}

// Read in the CSV records.
r := csv.NewReader(f)
records, err := r.ReadAll()
if err != nil {
    log.Fatal(err)
}

// Get the maximum value in the integer column.
var intMax int
for _, record := range records {

    // Parse the integer value.
    intVal, err := strconv.Atoi(record[0])
    if err != nil {
        log.Fatal(err)
    }

    // Replace the maximum value if appropriate.
    if intVal > intMax {
        intMax = intVal
    }
}

// Print the maximum value.
fmt.Println(intMax)
```

这将为 CSV 文件生成与所有整数值相同的正确结果：

```
$ go build
$ ./myprogram
3
```

但与前面的 Python 代码不同，当我们在输入 CSV 中遇到我们不期望的东西时，Go 代码会通知我们（对于删除值 3 的情况）：

```
$ go build
$ ./myprogram
2017/04/29 12:29:45 strconv.ParseInt: parsing "": invalid syntax
```

在这里，我们保持了完整性，并且我们可以确保以适合于我们的用例的方式处理缺少的值。

# 使用 Go 收集和组织数据的最佳实践

正如您在上一节中所看到的，Go 本身为我们提供了一个在数据收集、解析和组织中保持高水平完整性的机会。我们希望在为机器学习工作流准备数据时，确保充分利用 Go 的独特属性。

一般来说，Go 数据科学家/分析师在收集和组织数据时应遵循以下最佳实践。这些最佳实践旨在帮助您保持应用程序的完整性，并使您能够重现任何分析：

1.  **检查并强制执行预期类型**：这可能看起来很明显，但在使用动态类型语言时常常被忽略。虽然有点冗长，但显式地将数据解析为预期类型并处理相关错误可以为您节省大量的麻烦。
2.  **标准化和简化数据进出**：有许多第三方软件包用于处理某些类型的数据或与某些数据源的交互（本书将介绍其中一些）。但是，如果您将与数据源交互的方式标准化，特别是以`stdlib`的使用为中心，您可以开发可预测的模式，并在团队中保持一致性。一个很好的例子是选择使用`database/sql`进行数据库交互，而不是使用各种第三方 API 和 DSL。
3.  **版本您的数据**：机器学习模型根据您使用的训练数据、参数选择和输入数据产生极为不同的结果。因此，如果不同时对代码和数据进行版本控制，就不可能复制结果。我们将在本章后面讨论数据版本控制的适当技术。

如果你开始偏离这些一般原则，你应该立即停止。你可能为了方便而牺牲正直，这是一条危险的道路。我们将让这些原则引导我们通过这本书，并在下面的章节中考虑各种数据格式/来源。

# CSV 文件

CSV 文件可能不是大数据的常用格式，但作为从事机器学习的数据科学家或开发人员，您肯定会遇到这种格式。您可能需要将邮政编码映射到纬度/经度，并在 internet 上以 CSV 文件的形式找到它，或者您的销售团队会以 CSV 格式向您提供销售数据。无论如何，我们需要了解如何解析这些文件。

我们将在解析 CSV 文件时使用的主包是 Go 标准库中的`encoding/csv`。然而，我们还将讨论几个允许我们快速操作或转换 CSV 数据的包--`github.com/kniren/gota/dataframe`和`go-hep.org/x/hep/csvutil`。

# 从文件中读取 CSV 数据

让我们考虑一个简单的 CSV 文件，我们稍后将返回它，命名为 EndoT0.https://archive.ics.uci.edu/ml/datasets/iris [T2]。此 CSV 文件包括四个花卉测量浮动列和一个包含相应花卉种类的字符串列：

```
$ head iris.csv 
5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
5.0,3.6,1.4,0.2,Iris-setosa
5.4,3.9,1.7,0.4,Iris-setosa
4.6,3.4,1.4,0.3,Iris-setosa
5.0,3.4,1.5,0.2,Iris-setosa
4.4,2.9,1.4,0.2,Iris-setosa
4.9,3.1,1.5,0.1,Iris-setosa
```

导入`encoding/csv`后，我们首先打开 CSV 文件并创建一个 CSV 读取器值：

```
// Open the iris dataset file.
f, err := os.Open("../data/iris.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a new CSV reader reading from the opened file.
reader := csv.NewReader(f)
```

然后我们可以读入 CSV 文件的所有记录（对应于行）。这些记录作为`[][]string`导入：

```
// Assume we don't know the number of fields per line. By setting
// FieldsPerRecord negative, each row may have a variable
// number of fields.
reader.FieldsPerRecord = -1

// Read in all of the CSV records.
rawCSVData, err := reader.ReadAll()
if err != nil {
    log.Fatal(err)
}
```

我们还可以在无限循环中一次读取一条记录。只需确保检查文件的结尾（`io.EOF`，以便在读取所有数据后循环结束：

```
// Create a new CSV reader reading from the opened file.
reader := csv.NewReader(f)
reader.FieldsPerRecord = -1

// rawCSVData will hold our successfully parsed rows.
var rawCSVData [][]string

// Read in the records one by one.
for {

    // Read in a row. Check if we are at the end of the file.
    record, err := reader.Read()
    if err == io.EOF {
        break
    }

    // Append the record to our dataset.
    rawCSVData = append(rawCSVData, record)
}
```

如果您的 CSV 文件没有逗号分隔和/或 CSV 文件包含注释行，您可以利用`csv.Reader.Comma`和`csv.Reader.Comment`字段正确处理唯一格式的 CSV 文件。如果 CSV 文件中的字段是单引号，则可能需要添加一个帮助函数来修剪单引号并解析值。

# 处理意外字段

前面的方法可以很好地处理干净的 CSV 数据，但一般来说，我们不会遇到干净的数据。我们必须解析混乱的数据。例如，您可能会在 CSV 记录中发现意外的字段或字段数。这就是`reader.FieldsPerRecord`存在的原因。读卡器值的这个字段使我们能够轻松处理混乱的数据，如下所示：

```
4.3,3.0,1.1,0.1,Iris-setosa
5.8,4.0,1.2,0.2,Iris-setosa
5.7,4.4,1.5,0.4,Iris-setosa
5.4,3.9,1.3,0.4,blah,Iris-setosa
5.1,3.5,1.4,0.3,Iris-setosa
5.7,3.8,1.7,0.3,Iris-setosa
5.1,3.8,1.5,0.3,Iris-setosa
```

此版本的`iris.csv`文件在其中一行中有一个额外字段。我们知道每条记录应该有五个字段，所以让我们将`reader.FieldsPerRecord`值设置为`5`：

```
// We should have 5 fields per line. By setting
// FieldsPerRecord to 5, we can validate that each of the
// rows in our CSV has the correct number of fields.
reader.FieldsPerRecord = 5
```

然后，当我们从 CSV 文件中读取记录时，我们可以检查意外字段并保持数据的完整性：

```
// rawCSVData will hold our successfully parsed rows.
var rawCSVData [][]string

// Read in the records looking for unexpected numbers of fields.
for {

    // Read in a row. Check if we are at the end of the file.
    record, err := reader.Read()
    if err == io.EOF {
        break
    }

    // If we had a parsing error, log the error and move on.
    if err != nil {
        log.Println(err)
        continue
    }

    // Append the record to our dataset, if it has the expected
    // number of fields.
    rawCSVData = append(rawCSVData, record)
}
```

在这里，我们选择通过记录错误来处理错误，并且我们只将成功解析的记录收集到`rawCSVData`中。读者会注意到，这个错误可以用许多不同的方法来处理。重要的是，我们强迫自己检查数据的预期属性，并提高应用程序的完整性。

# 处理意外类型

我们刚刚看到 CSV 数据作为`[][]string`读入 Go。然而，Go 是静态类型的，这允许我们对每个 CSV 字段执行严格的检查。我们可以在解析每个字段以进行进一步处理时这样做。考虑一些混乱的数据，这些数据具有与列中其他值的类型不匹配的随机字段：

```
4.6,3.1,1.5,0.2,Iris-setosa
5.0,string,1.4,0.2,Iris-setosa
5.4,3.9,1.7,0.4,Iris-setosa
5.3,3.7,1.5,0.2,Iris-setosa
5.0,3.3,1.4,0.2,Iris-setosa
7.0,3.2,4.7,1.4,Iris-versicolor
6.4,3.2,4.5,1.5,
6.9,3.1,4.9,1.5,Iris-versicolor
5.5,2.3,4.0,1.3,Iris-versicolor
4.9,3.1,1.5,0.1,Iris-setosa
5.0,3.2,1.2,string,Iris-setosa
5.5,3.5,1.3,0.2,Iris-setosa
4.9,3.1,1.5,0.1,Iris-setosa
4.4,3.0,1.3,0.2,Iris-setosa
```

为了检查 CSV 记录中字段的类型，让我们创建一个`struct`变量来保存成功解析的值：

```
// CSVRecord contains a successfully parsed row of the CSV file.
type CSVRecord struct {
    SepalLength  float64
    SepalWidth   float64
    PetalLength  float64
    PetalWidth   float64
    Species      string
    ParseError   error
}
```

然后，在循环记录之前，让我们初始化这些值的一部分：

```
// Create a slice value that will hold all of the successfully parsed
// records from the CSV.
var csvData []CSVRecord
```

现在，当我们循环记录时，我们可以解析该记录的相关类型，捕获任何错误，并根据需要记录：

```

// Read in the records looking for unexpected types.
for {

    // Read in a row. Check if we are at the end of the file.
    record, err := reader.Read()
    if err == io.EOF {
        break
    }

    // Create a CSVRecord value for the row.
    var csvRecord CSVRecord

    // Parse each of the values in the record based on an expected type.
    for idx, value := range record {

        // Parse the value in the record as a string for the string column.
        if idx == 4 {

            // Validate that the value is not an empty string. If the
            // value is an empty string break the parsing loop.
            if value == "" {
                log.Printf("Unexpected type in column %d\n", idx)
                csvRecord.ParseError = fmt.Errorf("Empty string value")
                break
            }

            // Add the string value to the CSVRecord.
            csvRecord.Species = value
            continue
        }

        // Otherwise, parse the value in the record as a float64.
        var floatValue float64

        // If the value can not be parsed as a float, log and break the
        // parsing loop.
        if floatValue, err = strconv.ParseFloat(value, 64); err != nil {
            log.Printf("Unexpected type in column %d\n", idx)
            csvRecord.ParseError = fmt.Errorf("Could not parse float")
            break
        }

        // Add the float value to the respective field in the CSVRecord.
        switch idx {
        case 0:
            csvRecord.SepalLength = floatValue
        case 1:
            csvRecord.SepalWidth = floatValue
        case 2:
            csvRecord.PetalLength = floatValue
        case 3:
            csvRecord.PetalWidth = floatValue
        }
    }

    // Append successfully parsed records to the slice defined above.
    if csvRecord.ParseError == nil {
        csvData = append(csvData, csvRecord)
    }
}
```

# 使用数据帧处理 CSV 数据

如您所见，手动解析许多不同的字段并执行逐行操作可能会非常冗长和乏味。这绝对不是增加复杂性和引入大量非标准功能的借口。在大多数情况下，您仍应默认使用[T0]。

然而，数据框架的操作已被证明是（在数据科学界）处理表格数据的一种成功且有点标准化的方法。因此，在某些情况下，值得使用一些第三方功能来处理表格数据，例如 CSV 数据。例如，当您尝试筛选、子集和选择表格数据集的部分时，数据帧和相应的功能非常有用。在本节中，我们将介绍一款精彩的围棋`dataframe`套餐`github.com/kniren/gota/dataframe`：

```
import "github.com/kniren/gota/dataframe" 
```

要从 CSV 文件创建数据帧，我们打开一个带有`os.Open()`的文件，然后提供返回的指向`dataframe.ReadCSV()`函数的指针：

```
// Open the CSV file.
irisFile, err := os.Open("iris.csv")
if err != nil {
    log.Fatal(err)
}
defer irisFile.Close()

// Create a dataframe from the CSV file.
// The types of the columns will be inferred.
irisDF := dataframe.ReadCSV(irisFile)

// As a sanity check, display the records to stdout.
// Gota will format the dataframe for pretty printing.
fmt.Println(irisDF)
```

如果我们编译并运行这个 Go 程序，我们将看到一个漂亮的数据打印版本，其中包含解析过程中推断的类型：

```
$ go build
$ ./myprogram
[150x5] DataFrame

 sepal_length sepal_width petal_length petal_width species 
 0: 5.100000 3.500000 1.400000 0.200000 Iris-setosa
 1: 4.900000 3.000000 1.400000 0.200000 Iris-setosa
 2: 4.700000 3.200000 1.300000 0.200000 Iris-setosa
 3: 4.600000 3.100000 1.500000 0.200000 Iris-setosa
 4: 5.000000 3.600000 1.400000 0.200000 Iris-setosa
 5: 5.400000 3.900000 1.700000 0.400000 Iris-setosa
 6: 4.600000 3.400000 1.400000 0.300000 Iris-setosa
 7: 5.000000 3.400000 1.500000 0.200000 Iris-setosa
 8: 4.400000 2.900000 1.400000 0.200000 Iris-setosa
 9: 4.900000 3.100000 1.500000 0.100000 Iris-setosa
 ... ... ... ... ... 
 <float> <float> <float> <float> <string>
```

一旦我们将数据解析成一个`dataframe`，我们就可以很容易地过滤、子集和选择我们的数据：

```
// Create a filter for the dataframe.
filter := dataframe.F{
    Colname: "species",
    Comparator: "==",
    Comparando: "Iris-versicolor",
}

// Filter the dataframe to see only the rows where
// the iris species is "Iris-versicolor".
versicolorDF := irisDF.Filter(filter)
if versicolorDF.Err != nil {
    log.Fatal(versicolorDF.Err)
}

// Filter the dataframe again, but only select out the
// sepal_width and species columns.
versicolorDF = irisDF.Filter(filter).Select([]string{"sepal_width", "species"})

// Filter and select the dataframe again, but only display
// the first three results.
versicolorDF = irisDF.Filter(filter).Select([]string{"sepal_width", "species"}).Subset([]int{0, 1, 2})
```

这实际上只是触及了`github.com/kniren/gota/dataframe`包的表面。您可以合并数据集，输出为其他格式，甚至可以处理 JSON 数据。有关此软件包的更多信息，请访问自动生成的 GODOC，网址为[https://godoc.org/github.com/kniren/gota/dataframe](https://godoc.org/github.com/kniren/gota/dataframe) ，这通常是我们在书中讨论的任何包的良好实践。

# JSON

在一个大多数数据都是通过 web 访问的世界中，大多数工程组织都实现了一些微服务，我们将相当频繁地遇到 JSON 格式的数据。我们可能只需要在从 API 中提取一些随机数据时处理它，或者它实际上可能是驱动我们的分析和机器学习工作流的主要数据格式。

通常，当易用性是数据交换的主要目标时，使用 JSON。由于 JSON 是人类可读的，因此如果出现故障，很容易进行调试。请记住，我们希望在使用 Go 处理数据时保持数据处理的完整性，该过程的一部分是确保在可能的情况下，我们的数据是可解释和可读的。JSON 在实现这些目标方面非常有用（这就是为什么在许多情况下它也被用于日志记录）。

Go 在其标准库中通过`encoding/json`提供了非常出色的 JSON 功能。我们将在本书中使用此标准库功能。

# 解析 JSON

为了理解如何在 Go 中解析（即解组）JSON 数据，我们将使用 Citi Bike API（[中的一些数据 https://www.citibikenyc.com/system-data](https://www.citibikenyc.com/system-data) ），一项在纽约市运营的自行车共享服务。花旗自行车在[以 JSON 格式提供其自行车共享站网络的频繁更新运营信息 https://gbfs.citibikenyc.com/gbfs/en/station_status.json](https://gbfs.citibikenyc.com/gbfs/en/station_status.json) ：

```
{
  "last_updated": 1495252868,
  "ttl": 10,
  "data": {
    "stations": [
      {
        "station_id": "72",
        "num_bikes_available": 10,
        "num_bikes_disabled": 3,
        "num_docks_available": 26,
        "num_docks_disabled": 0,
        "is_installed": 1,
        "is_renting": 1,
        "is_returning": 1,
        "last_reported": 1495249679,
        "eightd_has_available_keys": false
      },
      {
        "station_id": "79",
        "num_bikes_available": 0,
        "num_bikes_disabled": 0,
        "num_docks_available": 33,
        "num_docks_disabled": 0,
        "is_installed": 1,
        "is_renting": 1,
        "is_returning": 1,
        "last_reported": 1495248017,
        "eightd_has_available_keys": false
      },

      etc...

      {
        "station_id": "3464",
        "num_bikes_available": 1,
        "num_bikes_disabled": 3,
        "num_docks_available": 53,
        "num_docks_disabled": 0,
        "is_installed": 1,
        "is_renting": 1,
        "is_returning": 1,
        "last_reported": 1495250340,
        "eightd_has_available_keys": false
      }
    ]
  }
}
```

要解析 Go 中的导入和这类数据，我们首先需要导入`encoding/json`（以及来自标准库的一些其他东西，例如`net/http`，因为我们将从前面提到的网站中提取这些数据）。我们还将定义模仿前面代码中所示 JSON 结构的`struct`：

```
import (
    "encoding/json"
    "fmt"
    "io/ioutil"
    "log"
    "net/http"
)

// citiBikeURL provides the station statuses of CitiBike bike sharing stations.
const citiBikeURL = "https://gbfs.citibikenyc.com/gbfs/en/station_status.json"

// stationData is used to unmarshal the JSON document returned form citiBikeURL.
type stationData struct {
    LastUpdated int `json:"last_updated"`
    TTL int `json:"ttl"`
    Data struct {
        Stations []station `json:"stations"`
    } `json:"data"`
}

// station is used to unmarshal each of the station documents in stationData.
type station struct {
    ID string `json:"station_id"`
    NumBikesAvailable int `json:"num_bikes_available"`
    NumBikesDisabled int `json:"num_bike_disabled"`
    NumDocksAvailable int `json:"num_docks_available"`
    NumDocksDisabled int `json:"num_docks_disabled"`
    IsInstalled int `json:"is_installed"`
    IsRenting int `json:"is_renting"`
    IsReturning int `json:"is_returning"`
    LastReported int `json:"last_reported"`
    HasAvailableKeys bool `json:"eightd_has_available_keys"`
}
```

注意这里有两点：（i）我们遵循 Go 习惯用法，避免使用带下划线的`struct`字段名，但是（ii）我们使用`json`结构标记将`struct`字段标记为 JSON 数据中相应的预期字段。

注意，要正确解析 JSON 数据，结构字段需要导出为字段。也就是说，字段需要以大写字母开头。`encoding/json`除非导出字段，否则无法使用 reflect 查看字段。

现在我们可以从 URL 获取 JSON 数据，并将其解组为新的`stationData`值。这将产生一个`struct`变量，其中各个字段填充了标记的 JSON 数据字段中的数据。我们可以通过打印一些与其中一个站点相关的数据来检查：

```
// Get the JSON response from the URL.
response, err := http.Get(citiBikeURL)
if err != nil {
    log.Fatal(err)
}
defer response.Body.Close()

// Read the body of the response into []byte.
body, err := ioutil.ReadAll(response.Body)
if err != nil {
    log.Fatal(err)
}

// Declare a variable of type stationData.
var sd stationData

// Unmarshal the JSON data into the variable.
if err := json.Unmarshal(body, &sd); err != nil {
    log.Fatal(err)
}

// Print the first station.
fmt.Printf("%+v\n\n", sd.Data.Stations[0])
```

当我们运行此程序时，我们可以看到我们的`struct`包含来自 URL 的解析数据：

```
$ go build
$ ./myprogram 
{ID:72 NumBikesAvailable:11 NumBikesDisabled:0 NumDocksAvailable:25 NumDocksDisabled:0 IsInstalled:1 IsRenting:1 IsReturning:1 LastReported:1495252934 HasAvailableKeys:false}
```

# JSON 输出

现在让我们假设我们的[T0]结构值中有 Citi Bike station 数据，我们希望将该数据保存到一个文件中。我们可以通过`json.marshal`实现这一点：

```
// Marshal the data.
outputData, err := json.Marshal(sd)
if err != nil {
    log.Fatal(err)
}

// Save the marshalled data to a file.
if err := ioutil.WriteFile("citibike.json", outputData, 0644); err != nil {
    log.Fatal(err)
}
```

# 类 SQL 数据库

尽管围绕有趣的 NoSQL 数据库和键值存储有很多炒作，但类似 SQL 的数据库仍然无处不在。每个数据科学家都会在某个时刻处理来自类似 SQL 的数据库（如 Postgres、MySQL 或 SQLite）的数据。

例如，我们可能需要查询 Postgres 数据库中的一个或多个表，以生成一组用于模型训练的功能。使用该模型进行预测或识别异常后，我们可以将结果发送到另一个数据库表，该数据库表驱动仪表板或其他报告工具。

Go 当然可以与所有流行的数据存储（如 SQL、NoSQL、key-value 等）进行良好的交互，但在这里，我们将重点讨论类似 SQL 的交互。我们将在整本书中利用`database/sql`进行这些交互。

# 连接到 SQL 数据库

在连接到类似 SQL 的数据库之前，我们需要做的第一件事是确定我们将与之交互的特定数据库，并导入相应的驱动程序。在以下示例中，我们将连接到 Postgres 数据库，并将使用`database/sql`的`github.com/lib/pq`数据库驱动程序。可通过空导入加载此驱动程序（带有相应注释）：

```
import (
    "database/sql"
    "fmt"
    "log"
    "os"

    // pq is the library that allows us to connect
    // to postgres with databases/sql.
    _ "github.com/lib/pq"
)
```

现在，假设您已将 Postgres 连接字符串导出到环境变量`PGURL`。我们可以通过以下代码轻松为我们的连接创建一个`sql.DB`值：

```
// Get the postgres connection URL. I have it stored in
// an environmental variable.
pgURL := os.Getenv("PGURL")
if pgURL == "" {
    log.Fatal("PGURL empty")
}

// Open a database value. Specify the postgres driver
// for databases/sql.
db, err := sql.Open("postgres", pgURL)
if err != nil {
    log.Fatal(err)
}
defer db.Close()
```

请注意，我们需要延迟此值的`close`方法。另外，请注意，创建此值并不意味着您已成功连接到数据库。这只是`database/sql`在某些操作（如查询）触发连接数据库时使用的值。

为了确保我们能够成功连接到数据库，我们可以使用`Ping`方法：

```
if err := db.Ping(); err != nil {
    log.Fatal(err)
}
```

# 查询数据库

现在我们知道了如何连接到数据库，让我们看看如何从数据库中获取数据。在本书中，我们将不介绍 SQL 查询和语句的细节。如果您不熟悉 SQL，我强烈建议您学习如何查询、插入等，但出于我们在这里的目的，您应该知道，与 SQL 数据库相关的操作基本上有两种类型：

*   `Query`操作选择、分组或聚合数据库中的数据，并将数据行返回给我们
*   `Exec`操作更新、插入或以其他方式修改数据库的状态，而无需返回存储在数据库中的部分数据

正如您所料，为了从我们的数据库中获取数据，我们将使用`Query`操作。为此，我们需要使用 SQL 语句字符串查询数据库。例如，假设我们有一个数据库存储了一组鸢尾花测量值（花瓣长度、花瓣宽度等），我们可以查询与特定鸢尾物种相关的一些数据，如下所示：

```
// Query the database.
rows, err := db.Query(`
    SELECT 
        sepal_length as sLength, 
        sepal_width as sWidth, 
        petal_length as pLength, 
        petal_width as pWidth 
    FROM iris
    WHERE species = $1`, "Iris-setosa")
if err != nil {
    log.Fatal(err)
}
defer rows.Close()
```

注意，这将返回一个指向`sql.Rows`值的指针，我们需要推迟关闭该行值。然后我们可以在行上循环并将数据解析为预期类型的值。我们对行使用`Scan`方法解析 SQL 查询返回的列，并将其打印到标准输出：

```
// Iterate over the rows, sending the results to
// standard out.
for rows.Next() {

    var (
        sLength float64
        sWidth float64
        pLength float64
        pWidth float64
    )

    if err := rows.Scan(&sLength, &sWidth, &pLength, &pWidth); err != nil {
        log.Fatal(err)
    }

    fmt.Printf("%.2f, %.2f, %.2f, %.2f\n", sLength, sWidth, pLength, pWidth)
}
```

最后，我们需要检查在处理行时可能发生的任何错误。我们希望保持数据处理的完整性，并且我们不能假设我们在所有行上循环时不会遇到错误：

```
// Check for errors after we are done iterating over rows.
if err := rows.Err(); err != nil {
    log.Fatal(err)
}
```

# 修改数据库

如前所述，还有另一种与数据库交互的风格，称为`Exec`。对于这些类型的语句，我们关心的是更新、添加或以其他方式修改数据库中一个或多个表的状态。我们使用相同类型的数据库连接，但不调用`db.Query`，而是调用`db.Exec`。

例如，假设我们要更新 iris 数据库表中的一些值：

```
// Update some values.
res, err := db.Exec("UPDATE iris SET species = 'setosa' WHERE species = 'Iris-setosa'")
if err != nil {
    log.Fatal(err)
}
```

但我们如何知道我们是否成功并改变了一些事情？这里返回的`res`函数允许我们查看表中有多少行受到更新的影响：

```
// See how many rows where updated.
rowCount, err := res.RowsAffected()
if err != nil {
    log.Fatal(err)
}

// Output the number of rows to standard out.
log.Printf("affected = %d\n", rowCount)
```

# 缓存

有时，我们的机器学习算法将通过来自外部源（例如 API）的数据进行训练和/或提供预测输入，即运行我们的建模或分析的应用程序不在本地的数据。此外，我们可能有各种数据集经常被访问，可能很快会再次被访问，或者可能需要在应用程序运行时提供这些数据集。

至少在某些情况下，将数据缓存在内存中或将数据嵌入运行应用程序的本地位置可能是有意义的。例如，如果您正在频繁地接触政府数据 API（通常具有高延迟），那么您可以考虑维护使用的人口普查数据的本地或内存缓存，这样您就可以避免不断地接触 API。

# 在内存中缓存数据

要在内存中缓存一系列值，我们将使用`github.com/patrickmn/go-cache`。有了这个包，我们可以创建一个键和相应值的内存缓存。我们甚至可以在缓存中为特定的键值对指定诸如生存时间之类的内容。

要创建新的内存缓存并在缓存中设置键值对，请执行以下操作：

```
// Create a cache with a default expiration time of 5 minutes, and which
// purges expired items every 30 seconds
c := cache.New(5*time.Minute, 30*time.Second)

// Put a key and value into the cache.
c.Set("mykey", "myvalue", cache.DefaultExpiration)
```

然后要从缓存中检索`mykey`的值，我们只需要使用`Get`方法：

```
v, found := c.Get("mykey")
if found {
    fmt.Printf("key: mykey, value: %s\n", v)
}
```

# 在磁盘上本地缓存数据

我们刚才看到的缓存在内存中。也就是说，缓存的数据在应用程序运行时存在并可访问，但一旦应用程序退出，数据就会消失。在某些情况下，当应用程序重新启动或退出时，您可能希望缓存的数据保持不变。您可能还希望备份缓存，这样就不必在没有相关数据缓存的情况下从头开始启动应用程序。

在这些场景中，您可以考虑使用本地的嵌入式缓存，如 AutoT0.正如所提到的，BoltDB 是这类应用程序的一个非常流行的项目，基本上由一个本地键值存储组成。要初始化其中一个本地键值存储，请执行以下操作：

```
// Open an embedded.db data file in your current directory.
// It will be created if it doesn't exist.
db, err := bolt.Open("embedded.db", 0600, nil)
if err != nil {
    log.Fatal(err)
}
defer db.Close()

// Create a "bucket" in the boltdb file for our data.
if err := db.Update(func(tx *bolt.Tx) error {
    _, err := tx.CreateBucket([]byte("MyBucket"))
    if err != nil {
        return fmt.Errorf("create bucket: %s", err)
    }
    return nil
}); err != nil {
    log.Fatal(err)
}
```

当然，您可以在 BoltDB 中有多个不同的数据桶，并使用除`embedded.db`之外的文件名。

接下来，假设您在内存中有一个字符串值映射，需要将其缓存在 BoltDB 中。要做到这一点，您需要在映射中的键和值范围内，更新您的 BoltDB：

```
// Put the map keys and values into the BoltDB file.
if err := db.Update(func(tx *bolt.Tx) error {
    b := tx.Bucket([]byte("MyBucket"))
    err := b.Put([]byte("mykey"), []byte("myvalue"))
    return err
}); err != nil {
    log.Fatal(err)
}
```

然后，要从 BoltDB 中获取值，您可以查看数据：

```
// Output the keys and values in the embedded
// BoltDB file to standard out.
if err := db.View(func(tx *bolt.Tx) error {
    b := tx.Bucket([]byte("MyBucket"))
    c := b.Cursor()
    for k, v := c.First(); k != nil; k, v = c.Next() {
        fmt.Printf("key: %s, value: %s\n", k, v)
    }
    return nil
}); err != nil {
    log.Fatal(err)
}
```

# 版本数据

如前所述，机器学习模型根据您使用的训练数据、参数选择和输入数据产生截然不同的结果。出于协作、创造性和合规性的原因，必须能够复制结果：

*   **协作**：不管你在社交媒体上看到什么，都没有数据科学和机器学习独角兽（即在数据科学和机器学习的各个领域都有知识和能力的人）。我们需要得到同事的审查并改进我们的工作，如果他们不能重现我们的模型结果和分析，这是不可能的。
*   **创造力**：我不知道你的情况，但我甚至记不起昨天做了什么。我们不能相信自己总是记住我们的推理和逻辑，特别是当我们处理机器学习工作流时。我们需要准确地跟踪我们正在使用的数据、我们创建的结果以及我们如何创建它们。这是我们能够不断改进模型和技术的唯一途径。
*   **遵从性**：最后，我们可能很快就无法选择机器学习中的数据版本控制和再现性。世界各地正在通过法律（例如，欧盟的**通用数据保护条例**（**GDPR**），允许用户对算法决策进行解释。如果我们没有一种可靠的方法来跟踪我们正在处理的数据和我们正在产生的结果，我们就无法希望遵守这些裁决。

有多个开源数据版本控制项目。其中一些重点关注数据的安全性和点对点分布式存储。其他人则专注于数据科学工作流程。在本书中，我们将重点介绍和利用厚皮动物（[http://pachyderm.io/](http://pachyderm.io/) ），一个用于数据版本控制和数据管道的开源框架。当我们在本书后面讨论生产部署和管理 ML 管道时，其中的一些原因将会很清楚。现在，我只总结一下 Pachydrm 的一些特性，这些特性使它成为基于 Go（和其他）ML 项目中数据版本控制的一个有吸引力的选择：

*   它有一个方便的 Go 客户端，`github.com/pachyderm/pachyderm/src/client`
*   能够对任何类型和格式的数据进行版本设置
*   用于版本化数据的灵活对象存储备份
*   与数据管道系统集成以驱动版本化的 ML 工作流

# 厚皮动物术语

考虑一下在 Pachyderm 中对数据进行版本控制，就像在 Git 中对代码进行版本控制一样。基本体是相似的：

*   **存储库**：这些是版本化的数据集合，类似于在 Git 存储库中拥有版本化的代码集合
*   **提交**：通过将数据提交到数据存储库，在 Pachydrm 中对数据进行版本控制
*   **分支**：这些轻量级指向某些提交或提交集（例如，主控指向最新的头提交）
*   **文件**：在 Pachydrm 中，数据在文件级别进行版本控制，Pachydrm 会自动采用重复数据消除等策略，以保持版本化数据的空间效率

尽管使用 Pachyderm 对数据进行版本控制感觉与使用 Git 对代码进行版本控制相似，但存在一些主要差异。例如，合并数据并不完全有意义。如果在数 PB 的数据上存在合并冲突，则任何人都无法解决这些冲突。此外，对于大型数据集，Git 协议通常不会节省空间。Pachydrm 使用其自己的内部逻辑来执行版本控制并处理版本控制的数据，该逻辑在缓存方面既节省空间又节省处理。

# 部署/安装 Pachydrm

我们将在书中的其他地方使用 Pachyderm 来创建版本数据和分布式 ML 工作流。Pachyderm 本身就是一款运行在 Kubernetes（[上的应用程序 https://kubernetes.io/](https://kubernetes.io/) ），并由您选择的对象存储支持。为了本书的目的，开发和实验，您可以轻松地在本地安装和运行 Pachydrm。它应该需要 5-10 分钟来安装，不需要太多的努力。本地安装说明可在[T2]的 Pachyderm 文档中找到 http://docs.pachyderm.io 。

当您准备在生产或部署模型中运行工作流时，您可以轻松部署一个生产就绪的 Pachydrm 集群，该集群的行为方式与本地安装完全相同。Pachydrm 可以部署到任何云，甚至可以部署到本地。

如前所述，Pachydrm 是一个开源项目，拥有一个活跃的用户群。如果您有疑问或需要帮助，可以访问[加入公共厚皮动物松弛频道 http://slack.pachyderm.io/](http://slack.pachyderm.io/) 。活跃的 Pachyderm 用户和 Pachyderm 团队本身将能够非常快速地回答您的问题。

# 为数据版本控制创建数据存储库

如果遵循 Pachydrm 文档中指定的本地安装 Pachydrm，则应具备以下功能：

*   Kubernetes 在您机器上的 Minikube 虚拟机中运行
*   [T0]命令行工具已安装并连接到您的 Pachydrm 群集

当然，如果您有一个在云中运行的生产集群，以下步骤仍然适用。您的`pachctl`将仅连接到远程群集。

我们将使用下面的`pachctl`**命令行界面**（**CLI**工具）演示数据版本控制功能（这是一个 Go 程序）。然而，如上所述，Pachyderm 有一个成熟的 Go 客户机。您可以创建存储库、提交数据，并且可以更直接地从您的 Go 程序中执行这些操作。该功能将在后面的第 9 章*部署和分发分析和模型*中演示。

要创建名为`myrepo`的数据存储库，可以运行以下代码：

```
$ pachctl create-repo myrepo
```

然后，您可以使用`list-repo`确认存储库存在：

```
$ pachctl list-repo
NAME CREATED SIZE 
myrepo 2 seconds ago 0 B
```

这个`myrepo`存储库是我们定义的数据集合，可以容纳版本化数据。现在，存储库中没有数据，因为我们还没有将任何数据放在那里。

# 将数据放入数据存储库

假设我们有一个简单的文本文件：

```
$ cat blah.txt 
This is an example file.
```

如果此文件是我们在 ML 工作流中使用的数据的一部分，我们应该对其进行版本设置。要在我们的存储库`myrepo`中对该文件进行版本设置，我们只需将其提交到该存储库中：

```
$ pachctl put-file myrepo master -c -f blah.txt 
```

`-c`标志指定我们希望 Pachydrm 打开一个新的提交，插入我们正在引用的文件，并一次性关闭提交。`-f`标志指定我们正在提供一个文件。

请注意，我们在这里向单个存储库的主分支提交单个文件。然而，Pachyderm API 具有难以置信的灵活性。我们可以在一次提交或多次提交中提交、删除或以其他方式修改多个版本化文件。此外，这些文件可以通过 URL、对象存储链接、数据库转储等进行版本控制。

作为健全性检查，我们可以确认我们的文件在存储库中的版本：

```
$ pachctl list-repo
NAME CREATED SIZE 
myrepo 10 minutes ago 25 B 
$ pachctl list-file myrepo master
NAME TYPE SIZE 
blah.txt file 25 B
```

# 从版本化数据存储库中获取数据

现在我们已经在 Pachydrm 中对数据进行了版本控制，我们可能想知道如何与该数据交互。主要的方法是通过厚皮动物数据管道（这将在本书后面讨论）。使用管道时与版本化数据交互的机制是一个简单的文件 I/O。

但是，如果我们想手动从 Pachydrm 中提取某些版本化数据集，以交互方式分析它们，那么我们可以使用`pachctl`CLI 获取数据：

```
$ pachctl get-file myrepo master blah.txt
This is an example file.
```

# 工具书类

CSV 数据：

*   `encoding/csv`文件：[https://golang.org/pkg/encoding/csv/](https://golang.org/pkg/encoding/csv/)
*   \12304；T0】文件：\12304；T1]https://godoc.org/github.com/kniren/gota/dataframe \12304；T2]

JSON 数据：

*   T0 文件：T1https://golang.org/pkg/encoding/json/ T2
*   比尔·肯尼迪关于 JSON 解码的博文：[https://www.goinggo.net/2014/01/decode-json-documents-in-go.html](https://www.goinggo.net/2014/01/decode-json-documents-in-go.html)
*   本·约翰逊的博客文章《走一遍》：`encoding/json`软件包：[https://medium.com/go-walkthrough/go-walkthrough-encoding-json-package-9681d1d37a8f](https://medium.com/go-walkthrough/go-walkthrough-encoding-json-package-9681d1d37a8f)

缓存：

*   `github.com/patrickmn/go-cache`文件：[https://godoc.org/github.com/patrickmn/go-cache](https://godoc.org/github.com/patrickmn/go-cache)
*   T0 文件：T1https://godoc.org/github.com/boltdb/bolt T2
*   关于 BoltDB 的信息和动机：[https://npf.io/2014/07/intro-to-boltdb-painless-performant-persistence/](https://npf.io/2014/07/intro-to-boltdb-painless-performant-persistence/)

厚皮动物：

*   一般文件：[http://docs.pachyderm.io](http://docs.pachyderm.io)
*   go client docs:【t0【https://godoc.org/github.com/pachyderm/pachyderm/src/client
*   公共用户团队注册：[http://docs.pachyderm.io](http://docs.pachyderm.io)

# 总结

在本章中，您学习了如何收集、组织和解析数据。这是开发机器学习模型的第一步，也是最重要的一步，但是如果我们不能获得一些关于数据的直觉并将其转换为标准形式进行处理，那么拥有数据并不能让我们走得很远。接下来，我们将讨论一些进一步构建数据（矩阵）和理解数据（统计和概率）的技术。